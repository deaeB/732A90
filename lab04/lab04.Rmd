---
title: "lab04"
author: "Shipeng Liu，Dongwei Ni"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library,include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
```
# Assignment 1:Computations with Metropolis–Hastings

## Task 1:Metropolis–Hastings Sampler
```{r 1.1.1}
target_pdf=function(x){
  if(x>0){
    return(x^5*exp(-x))
  }else{
    return(NA)
  }
}

f.MCMC.MH<-function(nstep,X0,props){
    vN<-1:nstep
    vX<-rep(X0,nstep);
    for (i in 2:nstep){
	    X<-vX[i-1]
	    Y<-rlnorm(1,meanlog=log(X),sd=props)
	    u<-runif(1,0,1)
	    a<-min(c(1,(target_pdf(Y)*dlnorm(X,meanlog=log(Y),sd=props))/(target_pdf(X)*dlnorm(Y,meanlog=log(X),sd=props))))
	if (u <=a){vX[i]<-Y}else{vX[i]<-X}    
    }
    return(data.frame("N"=vN,"X"=vX))
}

result=f.MCMC.MH(10000,1,1)

ggplot(data=result,aes(x=N,y=X))+geom_line()
```

*Convergence of the chain,and burn-in period*

It seems,the distribution of X doesn't change during the iteration,so the chain has converged,and there's no burn-in period.


## Task 2:Sampling by chi–square distribution

```{r 1.2.1}
mh_chi_square<-function(nstep,X0){
    vN<-1:nstep
    vX<-rep(X0,nstep);
    for (i in 2:nstep){
	    X<-vX[i-1]
	    Y<-rchisq(1,df=floor(X+1))
	    u<-runif(1,0,1)
	    a<-min(c(1,(target_pdf(Y)*dchisq(X,df=floor(Y+1)))/(target_pdf(X)*dchisq(Y,df=floor(X+1)))))
	if (u <=a){vX[i]<-Y}else{vX[i]<-X}    
    }
    return(data.frame("N"=vN,"X"=vX))
}

result_chi_square=mh_chi_square(10000,1)

ggplot(data=result_chi_square,aes(x=N,y=X))+geom_line()
```


## Task 3:Compare the result of Step 1 and 2 and make conclusion

We can clearly find that the chain in step 1 and step 2 both converged immediately (if you set nstep=100 and plot,there is only several point before converged),but variance is larger in step 2.So if we need to generate sample from the target distribution,we only need to take the sample which is generate from iteration $[10,\infty)$ .


## Task 4:Gelman–Rubin method to analyze convergence
```{r 1.4.1,warning=FALSE}
library(coda)
f1=mcmc.list()

for(i in 1:10){
  f1[[i]]=as.mcmc(mh_chi_square(1000,1)[,2])
}
print(gelman.diag(f1))
```

The Gelman–Rubin factor is around 1,which means the chain has converged.


## Task 5:Estimate Integral
We want to estimate integral$\int^\infty_0xf(x)dx$

where $\int^\infty_0f(x)dx=1$

Then,if $X\sim f(x)$:

$$\int^\infty_0xf(x)dx=E[X]$$
Estimate:
$$\overline E[X]=\frac 1n \sum^n_{i=1}x_i,\forall_ix_i \sim f(·)$$

So,using the sampling in step 1,we get:
```{r 1.5.1}
mean(result[,2])
```

Using the sampling in step 1,we get:
```{r 1.5.2}
mean(result_chi_square[,2])
```


## Task 6:Actual integral of gamma distribution

From wikipedia:https://en.wikipedia.org/wiki/Gamma_distribution

The expection of gamma distribution:
$$E[X]=k\theta$$

So,
$$\int^\infty_0xf(x)dx=E[X]=k\theta=6$$
where $k=6$ and $\theta=1$.

Our estimate value is very close to the actual integral of gamma distribution.

# Assignment 2:Gibbs Sampling

## Task 1:The dependence of Y on X

```{r 2.1.1}
load("chemical.RData")
chemical=data.frame("X"=X,"Y"=Y)
ggplot(data=chemical,aes(x=X,y=Y))+geom_point()

```

From the point of view of the distribution of sample points, it is more reasonable to use the logarithmic model.


## Task 2:Likelihood and Prior

#### Likelihood $p(\vec Y|\vec \mu)$: 

$$\begin{aligned}
p(\vec Y|\vec \mu)&=\prod^n_{i=1}{\frac 1{\sigma\sqrt{2\pi}} e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}} \\
&=(\frac 1{\sigma\sqrt {2\pi}})^{n}e^{-\sum_{i=1}^n\frac{(y_i-\mu_i)^2}{2\sigma^2}}
\end{aligned}$$
where $\sigma^2=0.2$

#### Prior$p(\vec\mu)$:

$$\begin{aligned}
p(\vec\mu)&=p(\mu_1)·p(\mu_2|\mu_1)·…·p(\mu_{n}|\mu_{n-1})  \\
&=1·\prod^{n-1}_{i=1}{\frac 1{\sigma\sqrt{2\pi}} e^{-\frac{(\mu_{i+1}-\mu_i)^2}{2\sigma^2}}} \\
&=(\frac 1{\sigma\sqrt {2\pi}})^{n-1}e^{-\sum_{i=1}^{n-1}\frac{(\mu_{i+1}-\mu_i)^2}{2\sigma^2}}
\end{aligned}$$
where $\sigma^2=0.2$


## Task 3:Posterior,conditional marginal distribution
Posterior$p(\vec\mu|\vec Y)$:

$$\begin{aligned}
p(\vec\mu|\vec{Y}) &\propto P(\vec{Y}|\vec{\mu})P(\vec{\mu}) \\
&\propto exp\Bigg{[}-\sum_{i=1}^n\frac{(y_i-\mu_i)^2}{2\sigma^2}\Bigg{]}exp\Bigg{[}-\sum_{i=1}^{n-1}\frac{(\mu_{i+1}-\mu_i)^2}{2\sigma^2}\Bigg{]} \\
&\propto exp \Bigg{[} -\frac{1}{2\sigma^2} \bigg{(} \sum^{n-1}_{i=1} \big{[} (\mu_i - \mu_{i+1})^2 + (\mu_i -  y_i)^2 \big{]} + (\mu_n - y_n)^2 \big{)} \Bigg{]}
\end{aligned}$$

Consider $p(\mu_1|\vec {\mu_{-1}},\vec Y)$:

$$p(\mu_1|\vec {\mu_{-1}},\vec Y)\propto exp\Bigg{[}-\frac 1{2\sigma^2}\bigg{(}(\mu_1-\mu_2)^2+(\mu_1-y_1)^2\bigg{)}\Bigg{]}$$ 
From Hint B we get:
$$p(\mu_1|\vec {\mu_{-1}},\vec Y)\propto exp\Bigg{[}-\frac 1{\sigma^2}\bigg{(}\mu_1-\frac {(\mu_2+y_1)}2\bigg{)}^2\Bigg{]} \sim N\bigg{(}\frac {\mu_2+y_1}2,\frac{\sigma^2}2 \bigg{)}$$

Consider $p(\mu_n|\vec {\mu_{-n}},\vec Y)$:

$$
p(\mu_n|\vec {\mu_{-n}},\vec Y)\propto exp\Bigg{[}-\frac 1{2\sigma^2}\bigg{(}(\mu_{n-1}-\mu_n)^2+(\mu_n-y_n)^2\bigg{)}\Bigg{]}
$$

From Hint B we get:
$$p(\mu_n|\vec {\mu_{-n}},\vec Y)\propto exp\Bigg{[}-\frac 1{\sigma^2}\bigg{(}\mu_n-\frac {(\mu_{n-1}+y_n)}2\bigg{)}^2\Bigg{]} \sim N\bigg{(}\frac {\mu_{n-1}+y_n}2,\frac{\sigma^2}2 \bigg{)}$$

Finally we consider $p(\mu_i|\vec {\mu_{-i}},\vec Y)$:

$$
p(\mu_i|\vec {\mu_{-i}},\vec Y)\propto exp\Bigg{[}-\frac 1{2\sigma^2}\bigg{(}(\mu_{i-1}-\mu_i)^2+(\mu_{i}-\mu_{i+1})^2+(\mu_i-y_i)^2\bigg{)}\Bigg{]}
$$


From Hint C we get:
$$
\begin{aligned}
p(\mu_i|\vec {\mu_{-i}},\vec Y)&\propto exp\Bigg{[}-\frac 1{2\sigma^2}\bigg{(}(\mu_{i}-\mu_{i-1})^2+(\mu_{i}-\mu_{i+1})^2+(\mu_i-y_i)^2\bigg{)}\Bigg{]} \\
&\propto exp\Bigg{[}-\frac{2\sigma^2}3 \bigg{(} \mu_i-\frac {(\mu_{i-1}+\mu_{i+1}+y_i)}3\bigg{)}^2\Bigg{]}\sim N\bigg{(}\frac {\mu_{i-1}+\mu_{i+1}+y_i}3,\frac{\sigma^2}3 \bigg{)}
\end{aligned}
$$


## Task 4:Implement a Gibbs sampler
```{r 2.4.1}
Gibbs=function(data,t_max){
  n=nrow(data)  #
  record=matrix(0,nrow=t_max,ncol = n)
  t=1
  while(t<=t_max){
    for(i in 1:n){
      if(i==1){
        record[t,i]=rnorm(1,(record[t,2]+data[1,2])/2,sqrt(0.2/2))
      }else if(i>1 && i<n){
        record[t,i]=rnorm(1,(record[t,i-1]+record[t,i+1]+data[i,2])/3,sqrt(0.2/3))
      }else if(i==n){
        record[t,i]=rnorm(1,(record[t,n-1]+data[n,2])/2,sqrt(0.2/2))
      }
    }
    if(t<t_max){
      record[t+1,]=record[t,]
    }
    t=t+1
  }
  return(record)
}

samp=Gibbs(chemical,1000)

```

#### The expected value of µ by Monte Carlo approach

The expected µ value:
```{r 2.4.2,echo=FALSE}
expect_mu=colMeans(samp)
expect_mu
```

#### Plot the expected value of µ versus X and Y versus X in the same graph
```{r 2.4.3,echo=FALSE}
chemical=chemical%>%mutate(mu=expect_mu)%>%group_by(X)%>%
  pivot_longer(Y:mu,names_to="slmp",values_to="ss")
ggplot(data=chemical,aes(x=X,y=ss))+geom_point(aes(color=slmp))+
  labs(x = "X", y = "Value",
       color = "Value") +
  scale_color_discrete(
    name = "Values:", 
    labels = c("Expected µ", "Y")
  )
```

It seems some of the noise have been removed,and the expected value of µ can catch the true underlying dependence between Y and X.


## Task 5:Converge

#### Trace plot for $\mu_n$
```{r 2.5.1}
samp_df=data.frame(samp)%>%mutate(sample=1:1000)
ggplot(samp_df,aes(x=sample,y=X50))+geom_line()

```

From the first sampling to the last time, $\mu_n$ always oscillates in a range, which means that this Markov chain reaches a stable condition and convergence at the beginning, and it has no Burn-in period










